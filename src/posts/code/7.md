---
icon: pen-to-square
date: 2025-03-26
category:
  - learn
tag:
  - python
---

# 爬虫code

## 1. Requests 库

### 1.1 安装与使用

Requests 是一个非常流行的第三方 HTTP 请求库。使用前需要先安装：

```bash
pip install requests
```

安装完成后，在代码中引入即可：

```python
import requests
```

### 1.2 基本用法：GET 请求

使用 `get` 方法可以发送 HTTP GET 请求，获取指定 URL 的响应内容。

```python
import requests

url = 'http://example.com'
response = requests.get(url)
print("状态码:", response.status_code)  # 打印状态码
print("响应内容:", response.text)       # 打印网页内容
```

在上面的代码中，`response` 对象包含了服务器返回的所有信息，包括状态码、响应头和响应内容。

### 1.3 Response 对象与 Request 对象

- **Response 对象**：包含服务器返回的响应信息。
  - `response.status_code`：HTTP 状态码，如 200、404 等。
  - `response.text`：响应内容，通常为字符串（文本、HTML 等）。
  - `response.json()`：如果响应内容为 JSON 格式，可调用该方法解析为 Python 数据结构。
  - `response.headers`：响应头信息，以字典形式呈现。

- **Request 对象**：通常在底层构造请求时使用，包含请求方法、URL、头信息等。对于日常使用，我们更多关注 Response 对象，而请求对象的细节由 Requests 库内部处理。

### 1.4 常用 HTTP 方法

Requests 提供了 7 个常用的方法，分别对应 HTTP 的不同请求方式：

- **get**：获取资源，如读取网页数据。  
- **post**：向服务器提交数据，例如提交表单。  
- **put**：更新服务器上的资源（一般为替换）。  
- **delete**：删除服务器上的资源。  
- **head**：仅获取响应头，常用于检查资源是否存在或获取元数据。  
- **options**：获取服务器支持的 HTTP 方法。  
- **patch**：对资源进行部分更新。

#### POST 请求示例

```python
import requests

url = 'http://httpbin.org/post'
data = {'name': 'Alice', 'age': 30}
response = requests.post(url, data=data)
print("POST 请求响应：", response.text)
```

#### PUT 请求示例

```python
import requests

url = 'http://httpbin.org/put'
data = {'update': 'true'}
response = requests.put(url, data=data)
print("PUT 请求响应：", response.text)
```

---

## 2. Beautiful Soup 库

Beautiful Soup 是一个用于解析 HTML 和 XML 文件的库，常与 Requests 配合使用来提取网页数据。

### 2.1 安装与使用

安装命令如下：

```bash
pip install beautifulsoup4
```

引入库并创建 BeautifulSoup 对象：

```python
from bs4 import BeautifulSoup
import requests

url = 'http://example.com'
response = requests.get(url)
# 使用 lxml 解析器解析 HTML 内容，也可使用 'html.parser'
soup = BeautifulSoup(response.text, 'lxml')

# 格式化输出 HTML 结构
print(soup.prettify())
```

### 2.2 基本元素与查找方法

- **Tag（标签）**：代表 HTML 中的标签，例如 `<a>`、`<div>` 等。
- **NavigableString**：标签内的文本内容。
- **BeautifulSoup 对象**：代表整个 HTML 文档。

常用的查找方法有：
- `find()`：查找第一个符合条件的标签。
- `find_all()`：查找所有符合条件的标签。

#### 示例：查找所有链接

```python
links = soup.find_all('a')
for link in links:
    print("链接文本：", link.get_text())
    print("链接地址：", link.get('href'))
```

---

## 3. re 库（正则表达式）

Python 内置的 `re` 模块用于字符串匹配和处理，常用于提取、替换或验证字符串格式。

### 3.1 基本概念

- **正则表达式**：由普通字符与特殊字符（如 `^`、`$`、`*`、`+`、`?` 等）构成，用于描述字符串模式。
- **贪婪匹配**：默认尽可能多地匹配字符。  
- **最小匹配**：在量词后添加 `?` 实现，匹配尽可能少的字符。

### 3.2 常用方法

- `re.compile(pattern)`：编译正则表达式，生成正则对象。
- `match()`：从字符串开始处匹配正则表达式。
- `search()`：扫描整个字符串并返回第一个匹配项。
- `findall()`：返回所有非重复匹配的列表。

#### 示例：匹配邮箱地址

```python
import re

text = "请联系邮箱：example@test.com 或者 admin@domain.com 获取更多信息。"
pattern = re.compile(r'[\w\.-]+@[\w\.-]+')
emails = pattern.findall(text)
print("匹配到的邮箱：", emails)
```

#### 示例：贪婪与非贪婪匹配

```python
text = "<div>内容1</div><div>内容2</div>"
# 贪婪匹配
greedy = re.search(r'<div>.*</div>', text)
print("贪婪匹配结果：", greedy.group())

# 非贪婪匹配
non_greedy = re.search(r'<div>.*?</div>', text)
print("非贪婪匹配结果：", non_greedy.group())
```

---

## 4. Scrapy 爬虫框架

Scrapy 是一个功能强大的爬虫框架，适合构建复杂的爬虫项目。它封装了网络请求、数据提取、数据存储等功能。

### 4.1 项目结构

Scrapy 项目通常包含如下目录和文件：
- **spiders/**：存放爬虫文件，每个爬虫都是一个 Python 类，继承自 `scrapy.Spider`。
- **items.py**：定义数据结构。
- **pipelines.py**：处理提取的数据，例如数据清洗或保存。
- **middlewares.py**：自定义中间件，可在请求和响应过程中做额外处理。
- **settings.py**：全局配置，包括并发、下载延迟、User-Agent 等。

### 4.2 常用命令

- **创建项目**：  
  ```bash
  scrapy startproject myproject
  ```
- **生成爬虫**：  
  ```bash
  cd myproject
  scrapy genspider example example.com
  ```
- **运行爬虫**：  
  ```bash
  scrapy crawl example
  ```

### 4.3 示例爬虫


```python
# 文件：myproject/spiders/example_spider.py
import scrapy

class ExampleSpider(scrapy.Spider):
    name = "example"
    allowed_domains = ["example.com"]
    start_urls = ['http://example.com']

    def parse(self, response):
        # 提取页面标题
        title = response.xpath('//title/text()').get()
        yield {'title': title}
```

运行爬虫后，Scrapy 会自动发送请求、解析响应，并将提取的数据通过管道处理（例如保存到文件或数据库）。

---

## 省流

- **Requests 库**：简单易用，适合发送 HTTP 请求，处理响应内容；常用方法包括 get、post、put 等。  
- **Beautiful Soup**：强大的 HTML/XML 解析工具，通过结构化方式提取网页内容。  
- **re 库**：内置正则表达式模块，用于复杂的字符串匹配和处理，支持贪婪与非贪婪匹配。  
- **Scrapy 框架**：适用于大规模爬虫项目，提供丰富的功能和灵活的配置，能高效地管理请求、解析数据和存储结果。
